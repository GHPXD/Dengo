"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ETL & ML TRAINING PIPELINE - DENGO PROJECT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Pipeline completo de coleta, tratamento e treinamento do modelo de prediÃ§Ã£o
de casos de dengue usando dados histÃ³ricos do InfoDengue (DATASUS/FIOCRUZ).

Autor: Dengo Team
Data: Dezembro 2025
VersÃ£o: 2.0.0 (Production-Ready)

Pipeline:
    1. ETL: ExtraÃ§Ã£o de dados histÃ³ricos (InfoDengue API)
    2. Feature Engineering: CriaÃ§Ã£o de variÃ¡veis preditivas
    3. ML Training: Treinamento com XGBoost
    4. Validation: MÃ©tricas de performance (MAE, RMSE, RÂ²)
    5. Export: Modelo + Metadata para produÃ§Ã£o
"""

import json
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple

import joblib
import numpy as np
import pandas as pd
import requests
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor

warnings.filterwarnings("ignore")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAÃ‡Ã•ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# GeocÃ³digo IBGE de Curitiba
GEOCODE = "4106902"

# PerÃ­odo de coleta (10 anos de histÃ³rico)
START_DATE = "2015-01-01"
END_DATE = "2024-12-31"

# URL da API InfoDengue (FIOCRUZ/DATASUS)
API_URL = f"https://info.dengue.mat.br/api/alertcity"

# Caminhos de saÃ­da
OUTPUT_DIR = Path(__file__).parent / "models"
MODEL_PATH = OUTPUT_DIR / "dengo_model.joblib"
METADATA_PATH = OUTPUT_DIR / "model_metadata.json"

# HiperparÃ¢metros do modelo
RANDOM_STATE = 42
TEST_YEAR = 2024
MIN_SAMPLES_TRAIN = 100


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ETAPA 1: ETL - EXTRAÃ‡ÃƒO DE DADOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def fetch_historical_data(geocode: str, start_year: int, end_year: int) -> pd.DataFrame:
    """
    Baixa dados histÃ³ricos de dengue da API InfoDengue.

    A API do InfoDengue fornece dados semanais de:
    - Casos notificados de dengue
    - Temperatura mÃ©dia/min/max
    - Umidade relativa
    - PrecipitaÃ§Ã£o acumulada
    - NÃ­vel de alerta epidemiolÃ³gico

    Args:
        geocode: CÃ³digo IBGE da cidade (ex: 4106902 = Curitiba)
        start_year: Ano inicial da coleta
        end_year: Ano final da coleta

    Returns:
        DataFrame com dados histÃ³ricos consolidados
    """
    print("=" * 80)
    print("ETAPA 1: EXTRAÃ‡ÃƒO DE DADOS (ETL)")
    print("=" * 80)
    print(f"ğŸ“¥ Coletando dados de dengue para {geocode} ({start_year}-{end_year})...")
    print(f"ğŸŒ API: InfoDengue (FIOCRUZ/DATASUS)")

    all_data = []

    for year in range(start_year, end_year + 1):
        print(f"  â†’ Baixando dados de {year}...", end=" ")

        try:
            # InfoDengue API endpoint
            url = f"{API_URL}?geocode={geocode}&disease=dengue&format=json&ew_start=1&ew_end=53&ey_start={year}&ey_end={year}"

            response = requests.get(url, timeout=30)
            response.raise_for_status()

            year_data = response.json()

            if year_data:
                all_data.extend(year_data)
                print(f"âœ“ {len(year_data)} registros")
            else:
                print("âš  Sem dados")

        except requests.exceptions.RequestException as e:
            print(f"âœ— Erro: {e}")
            continue

    if not all_data:
        raise ValueError("âŒ Nenhum dado foi coletado. Verifique a API ou o geocode.")

    df = pd.DataFrame(all_data)
    print(f"\nâœ… Total coletado: {len(df)} semanas epidemiolÃ³gicas")
    print(f"ğŸ“Š PerÃ­odo: {df['data_iniSE'].min()} atÃ© {df['data_iniSE'].max()}")

    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ETAPA 2: DATA CLEANING & FEATURE ENGINEERING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def clean_and_engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Limpa dados e cria features de engenharia para ML.

    TransformaÃ§Ãµes aplicadas:
        1. ConversÃ£o de tipos de dados
        2. Tratamento de valores ausentes
        3. CriaÃ§Ã£o de lags (sÃ©ries temporais)
        4. MÃ©dias mÃ³veis
        5. Sazonalidade (trigonomÃ©trica)
        6. Features climÃ¡ticas agregadas

    Args:
        df: DataFrame bruto da API

    Returns:
        DataFrame limpo e enriquecido com features
    """
    print("\n" + "=" * 80)
    print("ETAPA 2: LIMPEZA E FEATURE ENGINEERING")
    print("=" * 80)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2.1 ConversÃ£o de tipos
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ§¹ Convertendo tipos de dados...")

    df = df.copy()
    # Converter timestamp Unix (milissegundos) para datetime
    df["data_iniSE"] = pd.to_datetime(df["data_iniSE"], unit="ms")
    df = df.sort_values("data_iniSE").reset_index(drop=True)

    # Converte colunas numÃ©ricas
    numeric_cols = [
        "casos_est",
        "casos",
        "tempmin",
        "tempmed",
        "tempmax",
        "umidmin",
        "umidmed",
        "umidmax",
        "SE",
    ]

    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2.2 Tratamento de valores ausentes
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ”§ Tratando valores ausentes...")

    # Preenche casos com 0 (ausÃªncia de notificaÃ§Ã£o = sem casos)
    df["casos"] = df["casos"].fillna(0)
    df["casos_est"] = df["casos_est"].fillna(0)

    # Preenche temperatura com mÃ©dia da semana anterior
    df["tempmin"] = df["tempmin"].fillna(method="ffill").fillna(df["tempmin"].mean())
    df["tempmed"] = df["tempmed"].fillna(method="ffill").fillna(df["tempmed"].mean())
    df["tempmax"] = df["tempmax"].fillna(method="ffill").fillna(df["tempmax"].mean())

    # Preenche umidade com mÃ©dia da semana anterior
    df["umidmin"] = df["umidmin"].fillna(method="ffill").fillna(df["umidmin"].mean())
    df["umidmed"] = df["umidmed"].fillna(method="ffill").fillna(df["umidmed"].mean())
    df["umidmax"] = df["umidmax"].fillna(method="ffill").fillna(df["umidmax"].mean())

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2.3 Feature Engineering
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ”¬ Criando features de engenharia...")

    # LAG FEATURES (sÃ©ries temporais)
    df["casos_semana_anterior"] = df["casos"].shift(1).fillna(0)
    df["casos_2sem_anterior"] = df["casos"].shift(2).fillna(0)
    df["casos_3sem_anterior"] = df["casos"].shift(3).fillna(0)
    df["casos_4sem_anterior"] = df["casos"].shift(4).fillna(0)

    # ROLLING MEANS (mÃ©dias mÃ³veis)
    df["casos_media_4sem"] = df["casos"].rolling(window=4, min_periods=1).mean()
    df["temp_media_movel_4sem"] = df["tempmed"].rolling(window=4, min_periods=1).mean()
    df["umid_media_movel_4sem"] = df["umidmed"].rolling(window=4, min_periods=1).mean()

    # SAZONALIDADE (componentes trigonomÃ©tricas)
    df["semana_do_ano"] = df["SE"]
    df["sazonalidade_sen"] = np.sin(2 * np.pi * df["semana_do_ano"] / 52)
    df["sazonalidade_cos"] = np.cos(2 * np.pi * df["semana_do_ano"] / 52)

    # AMPLITUDES TÃ‰RMICAS E DE UMIDADE
    df["amplitude_termica"] = df["tempmax"] - df["tempmin"]
    df["amplitude_umidade"] = df["umidmax"] - df["umidmin"]

    # TENDÃŠNCIA (nÃºmero sequencial da semana)
    df["tendencia"] = range(len(df))

    # INTERAÃ‡Ã•ES (features polinomiais)
    df["temp_umid_interacao"] = df["tempmed"] * df["umidmed"]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2.4 RemoÃ§Ã£o de primeiras linhas (lag warmup)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Primeiras 4 semanas tÃªm lags incompletos
    df = df.iloc[4:].reset_index(drop=True)

    print(f"âœ… Features criadas: {len(df.columns)} colunas")
    print(f"ğŸ“Š Dataset final: {len(df)} semanas")

    # Verifica NaN antes de remover
    nan_counts = df.isna().sum()
    print(f"ğŸ” Debug - Colunas com NaN:")
    for col, count in nan_counts[nan_counts > 0].items():
        print(f"  {col}: {count} valores ausentes")

    # Remove linhas com NaN remanescentes (sÃ³ nas features importantes)
    critical_cols = ["casos", "tempmed", "umidmed", "data_iniSE"]
    df = df.dropna(subset=critical_cols)
    
    print(f"ğŸ“Š ApÃ³s remover NaN: {len(df)} semanas")

    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ETAPA 3: PREPARAÃ‡ÃƒO DOS DADOS PARA ML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def prepare_train_test_split(
    df: pd.DataFrame, test_year: int = 2024
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Separa dados em treino e teste (temporal split).

    EstratÃ©gia:
        - Treino: Todos os anos ANTES do test_year
        - Teste: Apenas o test_year
        - ValidaÃ§Ã£o temporal (nÃ£o shuffled)

    Args:
        df: DataFrame com features
        test_year: Ano para conjunto de teste

    Returns:
        Tupla (X_train, X_test, y_train, y_test)
    """
    print("\n" + "=" * 80)
    print("ETAPA 3: PREPARAÃ‡ÃƒO TREINO/TESTE")
    print("=" * 80)

    # Adiciona coluna de ano
    df["ano"] = df["data_iniSE"].dt.year

    # Separa treino e teste
    train_df = df[df["ano"] < test_year].copy()
    test_df = df[df["ano"] == test_year].copy()

    print(f"ğŸ“… Treino: {train_df['ano'].min()}-{train_df['ano'].max()} ({len(train_df)} semanas)")
    print(f"ğŸ“… Teste: {test_df['ano'].min()}-{test_df['ano'].max()} ({len(test_df)} semanas)")

    # Features para o modelo (remove colunas nÃ£o numÃ©ricas)
    feature_cols = [
        "tempmin",
        "tempmed",
        "tempmax",
        "umidmin",
        "umidmed",
        "umidmax",
        "casos_semana_anterior",
        "casos_2sem_anterior",
        "casos_3sem_anterior",
        "casos_4sem_anterior",
        "casos_media_4sem",
        "temp_media_movel_4sem",
        "umid_media_movel_4sem",
        "sazonalidade_sen",
        "sazonalidade_cos",
        "amplitude_termica",
        "amplitude_umidade",
        "tendencia",
        "temp_umid_interacao",
        "semana_do_ano",
    ]

    # Garante que todas as features existem
    feature_cols = [col for col in feature_cols if col in df.columns]

    X_train = train_df[feature_cols]
    y_train = train_df["casos"]

    X_test = test_df[feature_cols]
    y_test = test_df["casos"]

    print(f"âœ… Features selecionadas: {len(feature_cols)}")
    print(f"ğŸ“Š Shape treino: {X_train.shape}")
    print(f"ğŸ“Š Shape teste: {X_test.shape}")

    return X_train, X_test, y_train, y_test


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ETAPA 4: TREINAMENTO DO MODELO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def train_model(
    X_train: pd.DataFrame, y_train: pd.Series
) -> Tuple[XGBRegressor, StandardScaler, list]:
    """
    Treina modelo XGBoost com validaÃ§Ã£o cruzada.

    HiperparÃ¢metros otimizados para:
        - PrevisÃ£o de sÃ©ries temporais
        - Dados de saÃºde pÃºblica (volatilidade)
        - Evitar overfitting

    Args:
        X_train: Features de treino
        y_train: Target de treino

    Returns:
        Tupla (modelo treinado, scaler, feature_names)
    """
    print("\n" + "=" * 80)
    print("ETAPA 4: TREINAMENTO DO MODELO")
    print("=" * 80)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4.1 NormalizaÃ§Ã£o
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("âš–ï¸  Normalizando features (StandardScaler)...")

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4.2 Treinamento com XGBoost
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ¤– Treinando XGBoost Regressor...")

    model = XGBRegressor(
        n_estimators=200,  # NÃºmero de Ã¡rvores
        max_depth=6,  # Profundidade mÃ¡xima
        learning_rate=0.1,  # Taxa de aprendizado
        subsample=0.8,  # FraÃ§Ã£o de amostras por Ã¡rvore
        colsample_bytree=0.8,  # FraÃ§Ã£o de features por Ã¡rvore
        random_state=RANDOM_STATE,
        n_jobs=-1,  # Usa todos os cores
        verbosity=0,  # Silencioso
    )

    model.fit(X_train_scaled, y_train)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4.3 ValidaÃ§Ã£o Cruzada (Time Series Split)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ“Š ValidaÃ§Ã£o cruzada (Time Series Split)...")

    tscv = TimeSeriesSplit(n_splits=5)
    cv_scores = cross_val_score(
        model, X_train_scaled, y_train, cv=tscv, scoring="neg_mean_absolute_error", n_jobs=-1
    )

    cv_mae = -cv_scores.mean()
    cv_std = cv_scores.std()

    print(f"   MAE mÃ©dio (CV): {cv_mae:.2f} Â± {cv_std:.2f} casos")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4.4 Feature Importance
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    feature_importance = list(
        zip(X_train.columns, model.feature_importances_)
    )
    feature_importance.sort(key=lambda x: x[1], reverse=True)

    print("\nğŸ“ˆ Top 5 Features Mais Importantes:")
    for feat, imp in feature_importance[:5]:
        print(f"   {feat:30s} â†’ {imp:.4f}")

    return model, scaler, list(X_train.columns)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ETAPA 5: VALIDAÃ‡ÃƒO E MÃ‰TRICAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def evaluate_model(
    model: XGBRegressor,
    scaler: StandardScaler,
    X_test: pd.DataFrame,
    y_test: pd.Series,
) -> Dict[str, float]:
    """
    Avalia performance do modelo no conjunto de teste.

    MÃ©tricas calculadas:
        - MAE (Mean Absolute Error): Erro mÃ©dio em casos
        - RMSE (Root Mean Squared Error): Penaliza erros grandes
        - RÂ² (Coefficient of Determination): Qualidade do ajuste

    Args:
        model: Modelo treinado
        scaler: Scaler ajustado
        X_test: Features de teste
        y_test: Target de teste

    Returns:
        Dict com mÃ©tricas de performance
    """
    print("\n" + "=" * 80)
    print("ETAPA 5: VALIDAÃ‡ÃƒO DO MODELO")
    print("=" * 80)

    X_test_scaled = scaler.transform(X_test)
    y_pred = model.predict(X_test_scaled)

    # Calcula mÃ©tricas
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print("ğŸ“Š MÃ‰TRICAS DE PERFORMANCE:")
    print(f"   MAE (Erro MÃ©dio Absoluto):  {mae:.2f} casos")
    print(f"   RMSE (Raiz do Erro Quad.):  {rmse:.2f} casos")
    print(f"   RÂ² (Coef. DeterminaÃ§Ã£o):    {r2:.4f}")

    # AnÃ¡lise de resÃ­duos
    residuals = y_test - y_pred
    print(f"\nğŸ“‰ ANÃLISE DE RESÃDUOS:")
    print(f"   MÃ©dia dos resÃ­duos:         {residuals.mean():.2f}")
    print(f"   Desvio padrÃ£o:              {residuals.std():.2f}")
    print(f"   Min erro:                   {residuals.min():.2f}")
    print(f"   Max erro:                   {residuals.max():.2f}")

    return {"mae": mae, "rmse": rmse, "r2": r2}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ETAPA 6: EXPORTAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def save_model_and_metadata(
    model: XGBRegressor,
    scaler: StandardScaler,
    feature_names: list,
    metrics: Dict[str, float],
) -> None:
    """
    Salva modelo treinado e metadados.

    Arquivos gerados:
        - dengo_model.joblib: Modelo completo (XGBoost + Scaler + Features)
        - model_metadata.json: InformaÃ§Ãµes de treino e performance

    Args:
        model: Modelo treinado
        scaler: Scaler ajustado
        feature_names: Lista de nomes das features
        metrics: MÃ©tricas de performance
    """
    print("\n" + "=" * 80)
    print("ETAPA 6: EXPORTAÃ‡ÃƒO DO MODELO")
    print("=" * 80)

    # Cria diretÃ³rio de saÃ­da
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6.1 Salvar modelo (joblib)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"ğŸ’¾ Salvando modelo em: {MODEL_PATH}")

    model_artifact = {
        "model": model,
        "scaler": scaler,
        "feature_names": feature_names,
        "version": "2.0.0",
        "trained_at": datetime.now().isoformat(),
        "metrics": metrics,
    }

    joblib.dump(model_artifact, MODEL_PATH, compress=3)

    file_size_mb = MODEL_PATH.stat().st_size / (1024 * 1024)
    print(f"   âœ“ Tamanho: {file_size_mb:.2f} MB")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6.2 Salvar metadata (JSON)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"ğŸ“„ Salvando metadata em: {METADATA_PATH}")

    metadata = {
        "model_version": "2.0.0",
        "trained_at": datetime.now().strftime("%d/%m/%Y %H:%M:%S"),
        "trained_at_iso": datetime.now().isoformat(),
        "geocode": GEOCODE,
        "city": "Curitiba",
        "data_period": {"start": START_DATE, "end": END_DATE},
        "metrics": {
            "mae": round(metrics["mae"], 2),
            "rmse": round(metrics["rmse"], 2),
            "r2": round(metrics["r2"], 4),
        },
        "features_count": len(feature_names),
        "model_type": "XGBoost Regressor",
        "purpose": "PrediÃ§Ã£o de casos de dengue baseado em clima e histÃ³rico",
    }

    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"   âœ“ Metadata salvo")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE PRINCIPAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """
    Executa pipeline completo de ETL e treinamento.

    Passos:
        1. Coleta dados histÃ³ricos (InfoDengue API)
        2. Limpa e engenharia features
        3. Separa treino/teste
        4. Treina modelo XGBoost
        5. Valida performance
        6. Exporta modelo e metadata
    """
    print("\n")
    print("â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 20 + "DENGO - ML TRAINING PIPELINE" + " " * 30 + "â•‘")
    print("â•‘" + " " * 15 + "ETL + Feature Engineering + XGBoost" + " " * 28 + "â•‘")
    print("â•š" + "â•" * 78 + "â•")
    print()

    try:
        # ETAPA 1: Coleta de dados
        df_raw = fetch_historical_data(GEOCODE, 2015, 2024)

        # ETAPA 2: Limpeza e feature engineering
        df_clean = clean_and_engineer_features(df_raw)

        # ETAPA 3: PreparaÃ§Ã£o treino/teste
        X_train, X_test, y_train, y_test = prepare_train_test_split(df_clean, TEST_YEAR)

        # ValidaÃ§Ã£o de tamanho mÃ­nimo
        if len(X_train) < MIN_SAMPLES_TRAIN:
            raise ValueError(
                f"âŒ Dados insuficientes para treino: {len(X_train)} < {MIN_SAMPLES_TRAIN}"
            )

        # ETAPA 4: Treinamento
        model, scaler, feature_names = train_model(X_train, y_train)

        # ETAPA 5: ValidaÃ§Ã£o
        metrics = evaluate_model(model, scaler, X_test, y_test)

        # ETAPA 6: ExportaÃ§Ã£o
        save_model_and_metadata(model, scaler, feature_names, metrics)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RESUMO FINAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "=" * 80)
        print("âœ… PIPELINE CONCLUÃDO COM SUCESSO!")
        print("=" * 80)
        print(f"ğŸ¯ MAE Final: {metrics['mae']:.2f} casos")
        print(f"ğŸ“ Modelo salvo em: {MODEL_PATH}")
        print(f"ğŸ“„ Metadata em: {METADATA_PATH}")
        print(f"ğŸš€ Pronto para produÃ§Ã£o!")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ERRO NO PIPELINE: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
